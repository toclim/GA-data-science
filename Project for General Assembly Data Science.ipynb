{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for General Assembly Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Dataset\n",
    "\n",
    "Yelp review dataset with the following data fields:\n",
    "\n",
    "1. Business ID\n",
    "2. Date\n",
    "3. Review ID\n",
    "4. Stars\n",
    "5. Text\n",
    "6. Type\n",
    "7. User ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. To predict the 'Stars' rating by using the text reviews as features found in the 'Text' data field.\n",
    "2. Tuning parameters using **`RandomizedSearchCV`** and combining steps using **`Pipeline`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Reading in and exploring data\n",
    "2. Feature extraction using **`CountVectorizer`** and chaining cross validation using **`Pipeline`**\n",
    "3. Feature extraction using **`TfidfVectorizer`** and chaining cross validation using **`Pipeline`**\n",
    "4. Tuning **`CountVectorizer`** and classifiers using **`RandomizedSearchCV`**\n",
    "5. Tuning **`TfidfVectorizer`** and classifiers using **`RandomizedSearchCV`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading in and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/toclim/GA-data-science/master/yelp.csv'\n",
    "yelp = pd.read_csv(url)\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     749\n",
       "2     927\n",
       "3    1461\n",
       "4    3526\n",
       "5    3337\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    0\n",
       "date           0\n",
       "review_id      0\n",
       "stars          0\n",
       "text           0\n",
       "type           0\n",
       "user_id        0\n",
       "cool           0\n",
       "useful         0\n",
       "funny          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change into a 3-way classification problem by selecting only 1, 3 and 5 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "6  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "6  Drop what you're doing and drive here. After I...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
       "6  wFweIWhv2fREZV_dYkz_1g     7       7      4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_new = yelp[(yelp.stars==5) | (yelp.stars==3) | (yelp.stars==1)]\n",
    "yelp_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     749\n",
       "3    1461\n",
       "5    3337\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_new.stars.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction using **`CountVectorizer`** and chaining cross validation using **`Pipeline`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = yelp_new['text']\n",
    "y = yelp_new['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating **`CountVectorizer`** and classifiers using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78240718337046489"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb_vect = make_pipeline(vect, nb)\n",
    "cross_val_score(pipe_nb_vect, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79846061200128471"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logr_vect = make_pipeline(vect, logr)\n",
    "cross_val_score(pipe_logr_vect, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature extraction using **`TfidfVectorizer`** and chaining cross validation using **`Pipeline`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating **`TfidfVectorizer`** and classifiers using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60248863303626088"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb_tfidf = make_pipeline(tfidf, nb)\n",
    "cross_val_score(pipe_nb_tfidf, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7719560751365464"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logr_tfidf = make_pipeline(tfidf, logr)\n",
    "cross_val_score(pipe_logr_tfidf, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning **`CountVectorizer`** and classifiers using **`RandomizedSearchCV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['countvectorizer', 'multinomialnb'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb_vect.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__max_df': [0.1, 0.2, 0.3, 0.4],\n",
       " 'countvectorizer__min_df': [1, 2, 3],\n",
       " 'countvectorizer__ngram_range': [(1, 2)],\n",
       " 'countvectorizer__stop_words': [None, 'english'],\n",
       " 'multinomialnb__alpha': <scipy.stats._distn_infrastructure.rv_frozen at 0xb3d4208>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating search parameters for CountVectorizer and Naive Bayes\n",
    "nb_vect = {}\n",
    "nb_vect['countvectorizer__max_df'] = [0.1, 0.2, 0.3, 0.4]\n",
    "nb_vect['countvectorizer__min_df'] = [1, 2, 3]\n",
    "nb_vect['countvectorizer__stop_words'] = [None, 'english']\n",
    "nb_vect['countvectorizer__ngram_range'] = [(1, 2)]\n",
    "nb_vect['multinomialnb__alpha'] = sp.stats.uniform(scale=1)\n",
    "nb_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_nb_vect = RandomizedSearchCV(pipe_nb_vect, nb_vect, cv=5, scoring='accuracy', n_iter=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), p..., vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params={}, iid=True, n_iter=20, n_jobs=1,\n",
       "          param_distributions={'countvectorizer__max_df': [0.1, 0.2, 0.3, 0.4], 'countvectorizer__min_df': [1, 2, 3], 'countvectorizer__stop_words': [None, 'english'], 'countvectorizer__ngram_range': [(1, 2)], 'multinomialnb__alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000B3D4208>},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rand_nb_vect.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8123309897241753\n",
      "{'countvectorizer__max_df': 0.2, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': None, 'multinomialnb__alpha': 0.78592436822684564}\n"
     ]
    }
   ],
   "source": [
    "print(rand_nb_vect.best_score_)\n",
    "print(rand_nb_vect.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['countvectorizer', 'logisticregression'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logr_vect.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(logr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__max_df': [0.1, 0.2, 0.3, 0.4],\n",
       " 'countvectorizer__min_df': [1, 2, 3],\n",
       " 'countvectorizer__ngram_range': [(1, 2)],\n",
       " 'countvectorizer__stop_words': [None, 'english'],\n",
       " 'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen at 0xb6c59e8>,\n",
       " 'logisticregression__penalty': ['l1', 'l2']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating search parameters for CountVectorizer and Logistic Regression\n",
    "logr_vect = {}\n",
    "logr_vect['countvectorizer__max_df'] = [0.1, 0.2, 0.3, 0.4]\n",
    "logr_vect['countvectorizer__min_df'] = [1, 2, 3]\n",
    "logr_vect['countvectorizer__stop_words'] = [None, 'english']\n",
    "logr_vect['countvectorizer__ngram_range'] = [(1, 2)]\n",
    "logr_vect['logisticregression__penalty'] = ['l1', 'l2']\n",
    "logr_vect['logisticregression__C'] = sp.stats.uniform(scale=1)\n",
    "logr_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_logr_vect = RandomizedSearchCV(pipe_logr_vect, logr_vect, cv=5, scoring='accuracy', n_iter=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params={}, iid=True, n_iter=20, n_jobs=1,\n",
       "          param_distributions={'countvectorizer__max_df': [0.1, 0.2, 0.3, 0.4], 'countvectorizer__min_df': [1, 2, 3], 'countvectorizer__stop_words': [None, 'english'], 'countvectorizer__ngram_range': [(1, 2)], 'logisticregression__penalty': ['l1', 'l2'], 'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000B6C59E8>},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rand_logr_vect.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8062015503875969\n",
      "{'countvectorizer__max_df': 0.2, 'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.47838106564430904, 'logisticregression__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(rand_logr_vect.best_score_)\n",
    "print(rand_logr_vect.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning **`TfidfVectorizer`** and classifiers using **`RandomizedSearchCV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tfidfvectorizer', 'multinomialnb'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb_tfidf.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb__alpha': <scipy.stats._distn_infrastructure.rv_frozen at 0x123b8c50>,\n",
       " 'tfidfvectorizer__max_df': [0.1, 0.2, 0.3, 0.4],\n",
       " 'tfidfvectorizer__min_df': [1, 2, 3],\n",
       " 'tfidfvectorizer__ngram_range': [(1, 2)],\n",
       " 'tfidfvectorizer__norm': ['l1', 'l2', None],\n",
       " 'tfidfvectorizer__stop_words': [None, 'english']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating search parameters for TfidfVectorizer and Naive Bayes\n",
    "nb_tfidf = {}\n",
    "nb_tfidf['tfidfvectorizer__max_df'] = [0.1, 0.2, 0.3, 0.4]\n",
    "nb_tfidf['tfidfvectorizer__min_df'] = [1, 2, 3]\n",
    "nb_tfidf['tfidfvectorizer__stop_words'] = [None, 'english']\n",
    "nb_tfidf['tfidfvectorizer__ngram_range'] = [(1, 2)]\n",
    "nb_tfidf['tfidfvectorizer__norm'] = ['l1', 'l2', None]\n",
    "nb_tfidf['multinomialnb__alpha'] = sp.stats.uniform(scale=1)\n",
    "nb_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_nb_tfidf = RandomizedSearchCV(pipe_nb_tfidf, nb_tfidf, cv=5, scoring='accuracy', n_iter=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params={}, iid=True, n_iter=20, n_jobs=1,\n",
       "          param_distributions={'tfidfvectorizer__max_df': [0.1, 0.2, 0.3, 0.4], 'tfidfvectorizer__min_df': [1, 2, 3], 'tfidfvectorizer__stop_words': [None, 'english'], 'tfidfvectorizer__ngram_range': [(1, 2)], 'tfidfvectorizer__norm': ['l1', 'l2', None], 'multinomialnb__alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000000123B8C50>},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rand_nb_tfidf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7934018388318009\n",
      "{'multinomialnb__alpha': 0.018523381574367836, 'tfidfvectorizer__max_df': 0.2, 'tfidfvectorizer__min_df': 2, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__norm': 'l2', 'tfidfvectorizer__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "print(rand_nb_tfidf.best_score_)\n",
    "print(rand_nb_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tfidfvectorizer', 'logisticregression'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logr_tfidf.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen at 0x1271dac8>,\n",
       " 'logisticregression__penalty': ['l1', 'l2'],\n",
       " 'tfidfvectorizer__max_df': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
       " 'tfidfvectorizer__min_df': [1, 2, 3],\n",
       " 'tfidfvectorizer__ngram_range': [(1, 2)],\n",
       " 'tfidfvectorizer__norm': ['l1', 'l2', None],\n",
       " 'tfidfvectorizer__stop_words': [None, 'english']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating search parameters for TfidfVectorizer and Logistic Regression\n",
    "logr_tfidf = {}\n",
    "logr_tfidf['tfidfvectorizer__max_df'] = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "logr_tfidf['tfidfvectorizer__min_df'] = [1, 2, 3]\n",
    "logr_tfidf['tfidfvectorizer__stop_words'] = [None, 'english']\n",
    "logr_tfidf['tfidfvectorizer__ngram_range'] = [(1, 2)]\n",
    "logr_tfidf['tfidfvectorizer__norm'] = ['l1', 'l2', None]\n",
    "logr_tfidf['logisticregression__penalty'] = ['l1', 'l2']\n",
    "logr_tfidf['logisticregression__C'] = sp.stats.uniform(scale=1)\n",
    "logr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_logr_tfidf = RandomizedSearchCV(pipe_logr_tfidf, logr_tfidf, cv=5, scoring='accuracy', n_iter=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params={}, iid=True, n_iter=50, n_jobs=1,\n",
       "          param_distributions={'tfidfvectorizer__max_df': [0.1, 0.2, 0.3, 0.4, 0.5], 'tfidfvectorizer__min_df': [1, 2, 3], 'tfidfvectorizer__stop_words': [None, 'english'], 'tfidfvectorizer__ngram_range': [(1, 2)], 'tfidfvectorizer__norm': ['l1', 'l2', None], 'logisticregression__penalty': ['l1', 'l2'], 'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000001271DAC8>},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rand_logr_tfidf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8053001622498648\n",
      "{'logisticregression__C': 0.0093317967596888707, 'logisticregression__penalty': 'l2', 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 1, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__norm': None, 'tfidfvectorizer__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "print(rand_logr_tfidf.best_score_)\n",
    "print(rand_logr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal paramters based on **`RandomizedSearchCV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8123309897241753\n",
      "{'countvectorizer__max_df': 0.2, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': None, 'multinomialnb__alpha': 0.78592436822684564}\n"
     ]
    }
   ],
   "source": [
    "print(rand_nb_vect.best_score_)\n",
    "print(rand_nb_vect.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
